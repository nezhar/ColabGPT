{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDs5QkXJueJf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\" # https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Temporary Strorage\n",
        "conversation_storage = \"\"\n",
        "\n",
        "# OPTIONAL: Store messages in Drive\n",
        "# conversation_storage = \"/content/drive/MyDrive/ColabGPT\""
      ],
      "metadata": {
        "id": "MlD6Wc465b9z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Create conversation storage folder if needed and conversation file name\n",
        "if conversation_storage and not os.path.exists(conversation_storage):\n",
        "    os.makedirs(conversation_storage)\n",
        "convesation_name = f\"conversation_{time.strftime('%Y%m%d-%H%M%S')}.json\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "]\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Exiting...\")\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        # Prepare model inputs\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # Generate response with optimized parameters\n",
        "        generated_ids = model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=256, # Reduce max_new_tokens if memory is still an issue.\n",
        "            do_sample=True,     # Enable sampling for more diverse outputs\n",
        "            top_p=0.9,         # Nucleus sampling for better quality\n",
        "            temperature=0.7,    # Control randomness of generation\n",
        "        )\n",
        "\n",
        "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
        "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        print(\"Assistant:\")\n",
        "        for i in range(0, len(response), 80):\n",
        "            print(response[i:i+80])\n",
        "\n",
        "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        # Save conversation history only if storage is defined\n",
        "        with open(os.path.join(conversation_storage, convesation_name), \"w\") as f:\n",
        "            json.dump(messages, f, indent=4)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during generation: {e}\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "0LUIvxe_6Wzb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}