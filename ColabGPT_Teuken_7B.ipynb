{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDs5QkXJueJf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"openGPT-X/Teuken-7B-instruct-commercial-v0.4\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "model = model.to(device).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    use_fast=False,\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Language: bg, cs, da, de, el, en, es, et, fi, fr, ga, hr, hu, it, lt, lv, mt, nl, pl, pt, ro, sk, sl, sv\n",
        "language = \"de\"\n",
        "\n",
        "# Temporary Strorage\n",
        "conversation_storage = \"\"\n",
        "\n",
        "# OPTIONAL: Store messages in Drive\n",
        "# conversation_storage = \"/content/drive/MyDrive/ColabGPT\""
      ],
      "metadata": {
        "id": "MlD6Wc465b9z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Create conversation storage folder\n",
        "if conversation_storage and not os.path.exists(conversation_storage):\n",
        "    os.makedirs(conversation_storage)\n",
        "\n",
        "convesation_name = f\"conversation_{language}_{time.strftime('%Y%m%d-%H%M%S')}.json\"\n",
        "messages = []  # Initialize an empty list for the conversation\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    messages.append({\"role\": \"User\", \"content\": user_input})\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "      print(\"Exiting...\")\n",
        "      break\n",
        "\n",
        "    try:\n",
        "        prompt_ids = tokenizer.apply_chat_template(\n",
        "            messages, chat_template=language.upper(), tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        prediction = model.generate(\n",
        "            prompt_ids.to(model.device),\n",
        "            max_length=512,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            temperature=0.7,\n",
        "            num_return_sequences=1,\n",
        "        )\n",
        "        prediction_text = tokenizer.decode(prediction[0].tolist())\n",
        "\n",
        "        # Remove the prompt from the prediction\n",
        "        prompt_text = tokenizer.decode(prompt_ids[0].tolist())\n",
        "        prediction_text = prediction_text[len(prompt_text):].strip()\n",
        "\n",
        "        # Remove the </s> from the end of the prediction\n",
        "        prediction_text = prediction_text[:-4]\n",
        "\n",
        "        # print text as Assistant, max 80 chars per line\n",
        "        print(\"Assistant:\")\n",
        "        for i in range(0, len(prediction_text), 80):\n",
        "            print(prediction_text[i:i+80])\n",
        "\n",
        "        messages.append({\"role\": \"assistant\", \"content\": prediction_text})\n",
        "\n",
        "        # Save the conversation history as JSON\n",
        "        with open(os.path.join(conversation_storage, convesation_name), \"w\") as f:\n",
        "            json.dump(messages, f, indent=4)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during generation: {e}\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "f9GJ5ZyO0vGp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}